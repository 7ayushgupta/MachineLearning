{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 47765947.40102132\n",
      "1 55323557.172639236\n",
      "2 61010371.49357775\n",
      "3 48040988.586215876\n",
      "4 24305246.286257535\n",
      "5 8701618.476991344\n",
      "6 3528568.710683969\n",
      "7 2088612.9511737376\n",
      "8 1563363.1880765443\n",
      "9 1268601.7364685773\n",
      "10 1057663.044961592\n",
      "11 892722.053100422\n",
      "12 760012.2031077654\n",
      "13 651722.3261588097\n",
      "14 562631.4611492839\n",
      "15 488380.8099265325\n",
      "16 426078.17366267374\n",
      "17 373441.04028306203\n",
      "18 328742.9122064532\n",
      "19 290580.61769105814\n",
      "20 257771.516672853\n",
      "21 229440.6789846918\n",
      "22 204745.63846604511\n",
      "23 183233.87514181668\n",
      "24 164424.94128996704\n",
      "25 147944.62998064564\n",
      "26 133448.93892429766\n",
      "27 120643.1372144586\n",
      "28 109296.69319317062\n",
      "29 99208.79981616075\n",
      "30 90217.84946732451\n",
      "31 82192.95936754937\n",
      "32 75007.88749910292\n",
      "33 68559.518786175\n",
      "34 62772.7820837403\n",
      "35 57564.410169266135\n",
      "36 52864.67878258371\n",
      "37 48609.88646803316\n",
      "38 44752.93533102851\n",
      "39 41251.28072122173\n",
      "40 38064.7131418439\n",
      "41 35163.42981577587\n",
      "42 32516.57263058825\n",
      "43 30108.603688629155\n",
      "44 27906.09368097983\n",
      "45 25891.965068691887\n",
      "46 24044.16385463909\n",
      "47 22348.17376714191\n",
      "48 20787.99238905525\n",
      "49 19351.99477844178\n",
      "50 18028.82577414195\n",
      "51 16809.174048616267\n",
      "52 15682.548982137725\n",
      "53 14643.29055712147\n",
      "54 13681.475181933958\n",
      "55 12790.775562115501\n",
      "56 11965.697063630567\n",
      "57 11201.004933436456\n",
      "58 10491.599615015984\n",
      "59 9833.721771889464\n",
      "60 9222.713401482326\n",
      "61 8654.44223264396\n",
      "62 8125.164715000032\n",
      "63 7631.8714648690875\n",
      "64 7172.172226435582\n",
      "65 6743.364776298553\n",
      "66 6343.199362486122\n",
      "67 5969.619802775902\n",
      "68 5620.159744447363\n",
      "69 5293.427612571334\n",
      "70 4987.686332431327\n",
      "71 4701.507415942939\n",
      "72 4433.406947352956\n",
      "73 4182.1163315449885\n",
      "74 3947.162548359751\n",
      "75 3726.854791372118\n",
      "76 3520.146645959954\n",
      "77 3325.9511053223987\n",
      "78 3143.4247294549173\n",
      "79 2971.8214150945287\n",
      "80 2810.453804080371\n",
      "81 2658.681797627323\n",
      "82 2515.8539535624304\n",
      "83 2381.4118257657124\n",
      "84 2254.6944221405506\n",
      "85 2135.079568631866\n",
      "86 2022.359655252133\n",
      "87 1916.139695376252\n",
      "88 1815.956092044094\n",
      "89 1721.4092471179456\n",
      "90 1632.2246849143112\n",
      "91 1548.0100976484214\n",
      "92 1468.4514703455175\n",
      "93 1393.282165115455\n",
      "94 1322.2803917898636\n",
      "95 1255.1654157013386\n",
      "96 1191.7187689398104\n",
      "97 1131.6902046497776\n",
      "98 1074.884299242327\n",
      "99 1021.1765991236922\n",
      "100 970.3089210724718\n",
      "101 922.1283763801572\n",
      "102 876.4907420996528\n",
      "103 833.2647391717034\n",
      "104 792.3140550665137\n",
      "105 753.509529139948\n",
      "106 716.7139144040893\n",
      "107 681.8233469229008\n",
      "108 648.7252846825171\n",
      "109 617.3326272591516\n",
      "110 587.5435248948877\n",
      "111 559.2833548047543\n",
      "112 532.4681913008435\n",
      "113 506.993222774924\n",
      "114 482.8011794350347\n",
      "115 459.8306221240941\n",
      "116 438.02677042656336\n",
      "117 417.29586134794374\n",
      "118 397.59161341501164\n",
      "119 378.8653461386722\n",
      "120 361.06724172806344\n",
      "121 344.1483830433857\n",
      "122 328.0631912283759\n",
      "123 312.7636116493947\n",
      "124 298.20862759159127\n",
      "125 284.36472270038416\n",
      "126 271.1890862895185\n",
      "127 258.6521916602098\n",
      "128 246.72433028407698\n",
      "129 235.3719029197627\n",
      "130 224.56527817507256\n",
      "131 214.26884476511913\n",
      "132 204.46432362073222\n",
      "133 195.1339195920454\n",
      "134 186.2467585038801\n",
      "135 177.7786939148865\n",
      "136 169.70883598752982\n",
      "137 162.02030069866453\n",
      "138 154.69609132291902\n",
      "139 147.71392112836233\n",
      "140 141.0595822052578\n",
      "141 134.71641642221448\n",
      "142 128.6691633551065\n",
      "143 122.90387457152478\n",
      "144 117.40431744177859\n",
      "145 112.16196558757645\n",
      "146 107.16182757308948\n",
      "147 102.3928974928418\n",
      "148 97.8423271796966\n",
      "149 93.50060219118564\n",
      "150 89.35946002659992\n",
      "151 85.40633821386703\n",
      "152 81.6372549956717\n",
      "153 78.03672857015528\n",
      "154 74.60173486565553\n",
      "155 71.32232718128037\n",
      "156 68.19152646394953\n",
      "157 65.20324699942445\n",
      "158 62.35045801474245\n",
      "159 59.62503468940988\n",
      "160 57.02257191619138\n",
      "161 54.5379304323984\n",
      "162 52.16401720132944\n",
      "163 49.89638598049457\n",
      "164 47.7303827258875\n",
      "165 45.66122477468767\n",
      "166 43.6844684977443\n",
      "167 41.79646412393639\n",
      "168 39.99082729082459\n",
      "169 38.26577068686791\n",
      "170 36.61748115015955\n",
      "171 35.04134085739047\n",
      "172 33.53480961227775\n",
      "173 32.09485112846305\n",
      "174 30.718721183781806\n",
      "175 29.40290271813197\n",
      "176 28.144678012747455\n",
      "177 26.941475284880475\n",
      "178 25.79132759464846\n",
      "179 24.691806402729043\n",
      "180 23.640957098720868\n",
      "181 22.63501169059372\n",
      "182 21.672875382945925\n",
      "183 20.752385207688725\n",
      "184 19.872191098209434\n",
      "185 19.030298908824882\n",
      "186 18.224513400910766\n",
      "187 17.45367056181163\n",
      "188 16.71625206538662\n",
      "189 16.010916360679065\n",
      "190 15.33580722636426\n",
      "191 14.690063378275994\n",
      "192 14.07219840268963\n",
      "193 13.48046872914933\n",
      "194 12.914287181343237\n",
      "195 12.37240855137236\n",
      "196 11.853577429022213\n",
      "197 11.356981293870504\n",
      "198 10.881735797925518\n",
      "199 10.426748669951964\n",
      "200 9.99112877394386\n",
      "201 9.574014893359061\n",
      "202 9.174821816415689\n",
      "203 8.792791541309473\n",
      "204 8.426682077664877\n",
      "205 8.076138110650616\n",
      "206 7.740358260055977\n",
      "207 7.41889210659105\n",
      "208 7.11118701839221\n",
      "209 6.816310721309916\n",
      "210 6.533885536422019\n",
      "211 6.263405032940936\n",
      "212 6.004436680394363\n",
      "213 5.756359366224655\n",
      "214 5.518609991891433\n",
      "215 5.290831656234161\n",
      "216 5.072622877312218\n",
      "217 4.863650524077858\n",
      "218 4.663467854906345\n",
      "219 4.4716293544979315\n",
      "220 4.287768898192878\n",
      "221 4.111600322297397\n",
      "222 3.9429646961770066\n",
      "223 3.7812115285955454\n",
      "224 3.626216881399582\n",
      "225 3.477645250432965\n",
      "226 3.3352654556003265\n",
      "227 3.1988615461745447\n",
      "228 3.0680964850675494\n",
      "229 2.94272931862984\n",
      "230 2.8225632740272504\n",
      "231 2.7075123427136916\n",
      "232 2.5971747439293207\n",
      "233 2.4913339322121653\n",
      "234 2.389904974252018\n",
      "235 2.2926636718237177\n",
      "236 2.1994143158143045\n",
      "237 2.110058668324161\n",
      "238 2.0243647107563527\n",
      "239 1.9422486052778793\n",
      "240 1.8634865234710838\n",
      "241 1.7879620323914698\n",
      "242 1.7155271990879053\n",
      "243 1.6460672957464826\n",
      "244 1.579457120392206\n",
      "245 1.5155732057184994\n",
      "246 1.4543438118576386\n",
      "247 1.3956301396951898\n",
      "248 1.3392967501195503\n",
      "249 1.2852540501526395\n",
      "250 1.2334301329536055\n",
      "251 1.1837443703405504\n",
      "252 1.1360759458833392\n",
      "253 1.0903594256826172\n",
      "254 1.046492945772119\n",
      "255 1.004439157307494\n",
      "256 0.9640922793990819\n",
      "257 0.9253650780330176\n",
      "258 0.8882145864495851\n",
      "259 0.8525790895234838\n",
      "260 0.8183883482253894\n",
      "261 0.7856062257818449\n",
      "262 0.7541421283030523\n",
      "263 0.7239846583951386\n",
      "264 0.6950062235595281\n",
      "265 0.6672125014897428\n",
      "266 0.6405503430375991\n",
      "267 0.614955525995778\n",
      "268 0.5903976986096543\n",
      "269 0.5668365472760964\n",
      "270 0.544240767582671\n",
      "271 0.5225471765605026\n",
      "272 0.5017219357140028\n",
      "273 0.48173900997731545\n",
      "274 0.4625578391794393\n",
      "275 0.44415747518675996\n",
      "276 0.42649849616379926\n",
      "277 0.40954984192701066\n",
      "278 0.3932740429743167\n",
      "279 0.3776520153410026\n",
      "280 0.36266460942720025\n",
      "281 0.3482777256758599\n",
      "282 0.33446422011127735\n",
      "283 0.32120950742981547\n",
      "284 0.30849067565089183\n",
      "285 0.29627679288654657\n",
      "286 0.2845461890781223\n",
      "287 0.2732834750703263\n",
      "288 0.2624721665392752\n",
      "289 0.25209361710272377\n",
      "290 0.2421434383009285\n",
      "291 0.23257890601888673\n",
      "292 0.22339539059898592\n",
      "293 0.21458005045820816\n",
      "294 0.20611448270742722\n",
      "295 0.1979879232419557\n",
      "296 0.19018855873646362\n",
      "297 0.1826964858917575\n",
      "298 0.17550060983920154\n",
      "299 0.16859146121025165\n",
      "300 0.1619577238919641\n",
      "301 0.15558606271487796\n",
      "302 0.14947013435395093\n",
      "303 0.14360052484409422\n",
      "304 0.13795921841372139\n",
      "305 0.13254274421664908\n",
      "306 0.1273399088630491\n",
      "307 0.1223423124632189\n",
      "308 0.11754433655934249\n",
      "309 0.11293835981594194\n",
      "310 0.10851243214884539\n",
      "311 0.10426020294236867\n",
      "312 0.10017630422710361\n",
      "313 0.09625479384474997\n",
      "314 0.09249186004446344\n",
      "315 0.08887478974796706\n",
      "316 0.08539923537663464\n",
      "317 0.08206050529199818\n",
      "318 0.07885340970450724\n",
      "319 0.07577442930436844\n",
      "320 0.07281742985042061\n",
      "321 0.06997591049558588\n",
      "322 0.06724568597246053\n",
      "323 0.06462360367150768\n",
      "324 0.06210442679889231\n",
      "325 0.059684031917081534\n",
      "326 0.057360101809071244\n",
      "327 0.05512537963752024\n",
      "328 0.05297885091675636\n",
      "329 0.05091772880147948\n",
      "330 0.0489371955627461\n",
      "331 0.04703403108925171\n",
      "332 0.04520681107139608\n",
      "333 0.043449797640799974\n",
      "334 0.04176186803311227\n",
      "335 0.04013968039098624\n",
      "336 0.03858121426702542\n",
      "337 0.03708494249587711\n",
      "338 0.03564622264415576\n",
      "339 0.034263792445231735\n",
      "340 0.03293481298853149\n",
      "341 0.03165802331199226\n",
      "342 0.03043213933605676\n",
      "343 0.02925458527663076\n",
      "344 0.028121686325804116\n",
      "345 0.02703273523716366\n",
      "346 0.025986516338834108\n",
      "347 0.02498136177792675\n",
      "348 0.024015968817798782\n",
      "349 0.023087617529116516\n",
      "350 0.02219511316742628\n",
      "351 0.021337508642733156\n",
      "352 0.020513840039431876\n",
      "353 0.01972250165510079\n",
      "354 0.018961158771104276\n",
      "355 0.018229384639083336\n",
      "356 0.017526165906876754\n",
      "357 0.016850064657560636\n",
      "358 0.016201236401595355\n",
      "359 0.01557668559614453\n",
      "360 0.014976395731744745\n",
      "361 0.014399703748884163\n",
      "362 0.01384532402144779\n",
      "363 0.013312992515304331\n",
      "364 0.012800569510335032\n",
      "365 0.012307996565318828\n",
      "366 0.011834527720995549\n",
      "367 0.011379477584828785\n",
      "368 0.010942625488047388\n",
      "369 0.01052212261444406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 0.01011794795489527\n",
      "371 0.009729306371166877\n",
      "372 0.009355883401497221\n",
      "373 0.008997124274981283\n",
      "374 0.008651803175566481\n",
      "375 0.008319878430268152\n",
      "376 0.008000716202954165\n",
      "377 0.007694064314490925\n",
      "378 0.007399351019855574\n",
      "379 0.007115900657478592\n",
      "380 0.006843284512562408\n",
      "381 0.006581132375518561\n",
      "382 0.006329384941846981\n",
      "383 0.006087127905781004\n",
      "384 0.005854143738588299\n",
      "385 0.005630159996508557\n",
      "386 0.005414772338977993\n",
      "387 0.005207977699441345\n",
      "388 0.005008917240530983\n",
      "389 0.004817534121553294\n",
      "390 0.0046334015046149025\n",
      "391 0.004456478528739928\n",
      "392 0.00428639440678391\n",
      "393 0.004122719356581592\n",
      "394 0.003965361532289848\n",
      "395 0.003814032100840987\n",
      "396 0.003668645868156726\n",
      "397 0.00352877478450183\n",
      "398 0.003394268548206251\n",
      "399 0.003264838001895155\n",
      "400 0.0031404488698026922\n",
      "401 0.003020835807897399\n",
      "402 0.00290578656587498\n",
      "403 0.002795090645864025\n",
      "404 0.002688688578144388\n",
      "405 0.0025864158967343733\n",
      "406 0.002487981625692314\n",
      "407 0.0023933680025877016\n",
      "408 0.0023023201233058456\n",
      "409 0.0022148335505704775\n",
      "410 0.002130607719697014\n",
      "411 0.002049638201877681\n",
      "412 0.001971748128811433\n",
      "413 0.0018968671380943554\n",
      "414 0.0018248351411301515\n",
      "415 0.001755525682992203\n",
      "416 0.001688924661504546\n",
      "417 0.0016248323692824437\n",
      "418 0.0015631959599748317\n",
      "419 0.0015038729181223866\n",
      "420 0.001446815047534846\n",
      "421 0.0013919677125147664\n",
      "422 0.001339235656089779\n",
      "423 0.0012884673515861822\n",
      "424 0.001239630119495307\n",
      "425 0.0011926743015628718\n",
      "426 0.0011475383185762323\n",
      "427 0.0011040745413977524\n",
      "428 0.0010622558418650944\n",
      "429 0.0010220355889403907\n",
      "430 0.0009833879692286304\n",
      "431 0.0009461986203345352\n",
      "432 0.0009103920211097199\n",
      "433 0.0008759513945369008\n",
      "434 0.0008428644870646564\n",
      "435 0.0008109985835162495\n",
      "436 0.0007803444514889857\n",
      "437 0.000750852613182762\n",
      "438 0.0007224975174319091\n",
      "439 0.0006952056077270145\n",
      "440 0.0006689542055864854\n",
      "441 0.0006436916950820629\n",
      "442 0.0006193978881820475\n",
      "443 0.0005960371052142584\n",
      "444 0.0005735437719519274\n",
      "445 0.000551906560541477\n",
      "446 0.0005310989340020482\n",
      "447 0.000511070263357986\n",
      "448 0.0004917976317555272\n",
      "449 0.00047325518597530905\n",
      "450 0.00045543470722833845\n",
      "451 0.0004382786195570957\n",
      "452 0.00042176618894272796\n",
      "453 0.0004058759618377102\n",
      "454 0.0003905997344232932\n",
      "455 0.0003758940344288854\n",
      "456 0.0003617450620069509\n",
      "457 0.0003481265874906597\n",
      "458 0.00033503619937833766\n",
      "459 0.0003224292477051407\n",
      "460 0.0003103041748312276\n",
      "461 0.00029863729414608426\n",
      "462 0.00028741180736685805\n",
      "463 0.0002766042504124773\n",
      "464 0.00026620953710338835\n",
      "465 0.0002562096512643173\n",
      "466 0.0002465849356380078\n",
      "467 0.00023731850318770473\n",
      "468 0.00022840468562040851\n",
      "469 0.0002198347863866958\n",
      "470 0.00021158473788731512\n",
      "471 0.00020364056521781542\n",
      "472 0.00019599735304123497\n",
      "473 0.00018864711321776325\n",
      "474 0.00018156998908446957\n",
      "475 0.00017475753469478047\n",
      "476 0.00016820319822884714\n",
      "477 0.00016189917489236168\n",
      "478 0.00015583126908508504\n",
      "479 0.0001499912378423873\n",
      "480 0.0001443720335797392\n",
      "481 0.00013896257331518115\n",
      "482 0.0001337555085637566\n",
      "483 0.00012874484922320264\n",
      "484 0.0001239270890950026\n",
      "485 0.00011928607053197705\n",
      "486 0.00011481984439584253\n",
      "487 0.00011052508048011358\n",
      "488 0.00010639065989259653\n",
      "489 0.00010240899864030626\n",
      "490 9.857706465720605e-05\n",
      "491 9.489157279596638e-05\n",
      "492 9.134276532989929e-05\n",
      "493 8.792752098157122e-05\n",
      "494 8.463987591594466e-05\n",
      "495 8.147867453661876e-05\n",
      "496 7.84334617843529e-05\n",
      "497 7.550188732668879e-05\n",
      "498 7.268281669436015e-05\n",
      "499 6.996797982641056e-05\n"
     ]
    }
   ],
   "source": [
    "# Numpy revision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "N, D_in, H, D_out = 64,1000,100,10\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1324.4478759765625\n",
      "199 21.01254653930664\n",
      "299 0.5510542988777161\n",
      "399 0.018258845433592796\n",
      "499 0.0010332426754757762\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6b955a6d36fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Setting requires_grad=False indicates that we do not need to compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# with respect to these Tensors during the backward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
